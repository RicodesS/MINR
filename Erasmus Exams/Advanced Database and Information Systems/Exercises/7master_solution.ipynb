{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "from operator import add\n",
    "import time\n",
    "# from io import StringIO\n",
    "# import csv\n",
    "import re\n",
    "# import sys\n",
    " \n",
    "# initialise Spark Session\n",
    "spark = SparkSession.builder.appName(\"cite\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "execution_time_file = open('execution_time_file','a') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a line from the file papers_vocab.txt\n",
    "# comma to separate ID and vocab, space to separate vocabularies.\n",
    "def parse_papers_count(line):\n",
    "    if not line:\n",
    "        return dict()\n",
    "    papersCountRaw = line.split(' ')\n",
    "    papersCount = dict()\n",
    "    for pcRaw in papersCountRaw:\n",
    "        paper, count = pcRaw.split(':')\n",
    "        papersCount[paper] = int(count)\n",
    "    return papersCount\n",
    "\n",
    "# Parse a line from the file users_libraries.txt\n",
    "# semi-colon to separate user hash id with library, comma to separate the IDs in the library.\n",
    "def parse_users_libraries(line):\n",
    "    if not line:\n",
    "        return\n",
    "    \n",
    "    userHash, libraryRAW = line.split(';')\n",
    "    library = [int(paper_id) for paper_id in libraryRAW.split(',')]\n",
    "    return userHash, library    \n",
    "\n",
    "\n",
    "# Parse a line from the file papers.csv. Comma seperated. Each line has 15 fields.\n",
    "# The first is paper_id, the last is 'abstract' of a paper.\n",
    "def parse_papers(line):\n",
    "    if not line:\n",
    "        return\n",
    "    # Old code:\n",
    "    # papersInfo = csv.reader([line], delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    # papersInfoList = list(papersInfo)\n",
    "    # return papersInfoList[0][0], papersInfoList[0][14]\n",
    "    \n",
    "    # Updated code:\n",
    "    papersInfo = csv.reader([line.replace(\"\\0\", \"\")], delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    papersInfoList = next(papersInfo)\n",
    "    # paper_id, abstract (the last element in the list)\n",
    "    return papersInfoList[0], papersInfoList[-1]\n",
    "    \n",
    "    \n",
    "    # Using Regular Expression\n",
    "    # papersInfo = re.match('(?P<paper_id>[^,]*),([^,]*,){13}\\\\\\\"*(?P<abstract>.*)\\\\\\\"*', line)\n",
    "    # paper_id, abstract\",\n",
    "    # return papersInfo.group('paper_id'), papersInfo.group('abstract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 6.2 Loading the data into an RDD\n",
    "Load the dataset carrying out the following steps:\n",
    "* `userRatingsRDD` : create a pair RDD from <i>user_libraries.txt</i> using the user hash as the key and the liked paper(s) (citeulike doc id) as the value(s).\n",
    "* `paperTermsRDD` : create a pair RDD from <i>papers.csv</i> using the citeulike doc id as the key and the words contained in the abstract as the value(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# load user libraries\n",
    "userRatingsRDD = sc.textFile('citeulike/users_libraries.txt')\n",
    "# key - user_hash id, values - list of paper_ids\n",
    "userRatingsRDD = userRatingsRDD.map(parse_users_libraries)\n",
    "\n",
    "execution_time_file.write('Loading of userRatingsRDD: --- %s seconds --- \\n' % (time.time() - start_time)) \n",
    "\n",
    "start_time = time.time()\n",
    "# load paper terms \n",
    "paperTermsRDD = sc.textFile('citeulike/papers.csv')\n",
    "\n",
    "# key - paper_id, value - abstract\n",
    "allPaperTermsRDD = paperTermsRDD.map(parse_papers)\n",
    "\n",
    "# filter empty abstracts\n",
    "paperTermsRDD = allPaperTermsRDD.filter(lambda x: x[1] is not '')\n",
    "\n",
    "# key - paper_id, values - list of words \n",
    "paperTermsRDD = paperTermsRDD.map(lambda x: (int(x[0]), x[1].split(' ')))\n",
    "\n",
    "execution_time_file.write('Loading of paperTermsRDD: --- %s seconds --- \\n' % (time.time() - start_time)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invalid Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'papersSchema = StructType([\\n    #  name, dataType, nullable\\n    StructField(\"paper_id\", IntegerType(), False),\\n    StructField(\"type\", StringType(), True),\\n    StructField(\"journal\", StringType(), True),\\n    StructField(\"book_title\", StringType(), True),\\n    StructField(\"series\", StringType(), True),\\n    StructField(\"publisher\", StringType(), True),\\n    StructField(\"pages\", StringType(), True),\\n    StructField(\"volume\", StringType(), True),\\n    StructField(\"number\", StringType(), True),\\n    StructField(\"year\", StringType(), True),\\n    StructField(\"month\", StringType(), True),\\n    StructField(\"postedat\", StringType(), True),\\n    StructField(\"address\", StringType(), True),\\n    StructField(\"title\", StringType(), True),\\n    StructField(\"abstract\", StringType(), True)\\n])\\n\\npaperTerms = spark.read.csv(\"citeulike/papers.csv\", header = False, schema = papersSchema)\\n# remove empty abstracts\\npaperTerms = paperTerms.filter(paperTerms.abstract.isNotNull())\\n# transform to RDD\\npaperTermsRDD = paperTerms.select(\\'paper_id\\' ,\\'abstract\\').rdd'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading of papers into DF and then convertion to RDD is inappropriate\n",
    "# Provide schema while loading papers\n",
    "'''papersSchema = StructType([\n",
    "    #  name, dataType, nullable\n",
    "    StructField(\"paper_id\", IntegerType(), False),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"journal\", StringType(), True),\n",
    "    StructField(\"book_title\", StringType(), True),\n",
    "    StructField(\"series\", StringType(), True),\n",
    "    StructField(\"publisher\", StringType(), True),\n",
    "    StructField(\"pages\", StringType(), True),\n",
    "    StructField(\"volume\", StringType(), True),\n",
    "    StructField(\"number\", StringType(), True),\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"postedat\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"abstract\", StringType(), True)\n",
    "])\n",
    "\n",
    "paperTerms = spark.read.csv(\"citeulike/papers.csv\", header = False, schema = papersSchema)\n",
    "# remove empty abstracts\n",
    "paperTerms = paperTerms.filter(paperTerms.abstract.isNotNull())\n",
    "# transform to RDD\n",
    "paperTermsRDD = paperTerms.select('paper_id' ,'abstract').rdd'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.3 Joining Collectios\n",
    "Implement a simple function that computes for each user the top-k most frequent words appearing in\n",
    "the papers she likes. Exclude the stop words listed in <i>stopwords en.txt </i>.\n",
    "Store the results into a file which contains in each line the user hash and the list of her retrieved words\n",
    "sorted by frequency:\n",
    "> user hash, word k, word k-1, word k-2,..., word 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load and broadcast the stopwords\n",
    "stopWords = [line.rstrip('\\n') for line in open('citeulike/stopwords_en.txt')]\n",
    "stopWordsBroadcast = sc.broadcast(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# key - user_hash_id, value - paper_id\n",
    "userRatingRDD = userRatingsRDD.flatMapValues(lambda x: x) \n",
    "\n",
    "# Exchange key and value. The key becomes paper_id, the value - user_hash_id\n",
    "ratingUserRDD = userRatingRDD.map(lambda x:(x[1], x[0]))\n",
    "\n",
    "# key - paper_id, value - a word\n",
    "paperTermRDD = paperTermsRDD.flatMapValues(lambda x: x) \n",
    "# remove stop words before joining\n",
    "paperTermRDD = paperTermRDD.filter(lambda x: x[1] not in stopWordsBroadcast.value)\n",
    "\n",
    "# user_hash_id, a word\n",
    "userWords = ratingUserRDD.join(paperTermRDD).map(lambda x: (x[1][0], x[1][1]))\n",
    "userWords = userWords.map(lambda x: (x[0] + ' ' + x[1], 1))\n",
    "countedUserWords = userWords.reduceByKey(add)\n",
    "\n",
    "# key - user_hash_id, value - (word , word_count)\n",
    "countedUserWords = countedUserWords.map(lambda x: (x[0].split(' ')[0], [(x[0].split(' ')[1], x[1])]))\n",
    "countedUserWords = countedUserWords.reduceByKey(lambda a, b: a + b)\n",
    "countedUserWords = countedUserWords.map(lambda x: (x[0], sorted(x[1], key=lambda y: y[1], reverse=True)[:10] ))\n",
    "execution_time_file.write('Preparation and joining of RDD: --- %s seconds --- \\n' % (time.time() - start_time)) \n",
    "\n",
    "# save in a file\n",
    "start_time = time.time()\n",
    "countedUserWords.saveAsTextFile(\"user_top_10_rdd\")\n",
    "execution_time_file.write('Save joined RDD in a file: --- %s seconds --- \\n' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.4 Basic Analysis for Recommender Systems\n",
    "Retrieve the following information:\n",
    "* number of (distinct) user, number of (distinct) items, and number of ratings\n",
    "* min number of ratings a user has given\n",
    "* max number of ratings a user has given\n",
    "* average number of ratings of users\n",
    "* standard deviation for ratings of users\n",
    "* min number of ratings an item has received\n",
    "* max number of ratings an item has received\n",
    "* average number of ratings of items\n",
    "* standard deviation for ratings of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# number of users\n",
    "usersCount = userRatingsRDD.count()\n",
    "\n",
    "# number of items - allPaperTermsRDD (papers terms without filtering of empty abstracts)\n",
    "itemsCount = allPaperTermsRDD.count()\n",
    "\n",
    "# number of ratings\n",
    "#ratingUserRDD -> paperid, the value - user_hash_id\n",
    "ratingsPerUserCount = ratingUserRDD.count()\n",
    "\n",
    "# userRatingsRDD -> key - user_hash_id, values - list of paper_ids\n",
    "# count number of liked documents per user\n",
    "ratingsCountPerUser = userRatingsRDD.map(lambda x: len(x[1]))\n",
    "minRatingsPerUser = ratingsCountPerUser.min()\n",
    "maxRatingsPerUser = ratingsCountPerUser.max()\n",
    "avgRatingsPerUser = ratingsCountPerUser.mean()\n",
    "stdRatingsPerUser = ratingsCountPerUser.stdev()\n",
    "\n",
    "\n",
    "# ratingUserRDD -> paper_id, the value - user_hash_id\n",
    "# group by paper_id and count the number of users rated this item\n",
    "ratingsCountPerItem = ratingUserRDD.groupBy(lambda x: x[0]).map(lambda x: len(x[1])) \n",
    "minRatingsPerItem = ratingsCountPerItem.min()\n",
    "maxRatingsPerItem = ratingsCountPerItem.max()\n",
    "avgRatingsPerItem = ratingsCountPerItem.mean()\n",
    "stdRatingsPerItem = ratingsCountPerItem.stdev()\n",
    "\n",
    "execution_time_file.write('Basic analysis over RDDs: --- %s seconds --- \\n' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.5 Loading the dataset into Data Frames\n",
    "Loading the dataset into Data Frames, leveraging the structure of the data. The result should be a database\n",
    "which contains one table for each file (you are free to choose the schema of the table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load papers\n",
    "# provide schema while loading papers\n",
    "start_time = time.time()\n",
    "papersSchema = StructType([\n",
    "    #  name, dataType, nullable\n",
    "    StructField(\"paper_id\", IntegerType(), False),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"journal\", StringType(), True),\n",
    "    StructField(\"book_title\", StringType(), True),\n",
    "    StructField(\"series\", StringType(), True),\n",
    "    StructField(\"publisher\", StringType(), True),\n",
    "    StructField(\"pages\", StringType(), True),\n",
    "    StructField(\"volume\", StringType(), True),\n",
    "    StructField(\"number\", StringType(), True),\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"postedat\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"abstract\", StringType(), True)\n",
    "])\n",
    "\n",
    "papers = spark.read.csv(\"citeulike/papers.csv\", header = False, schema = papersSchema)\n",
    "\n",
    "execution_time_file.write('Loading of papers into DF: --- %s seconds --- \\n' % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "# load user libraries\n",
    "usersLibrariesSchema = StructType([\n",
    "    #  name, dataType, nullable\n",
    "    StructField(\"user_hash_id\", StringType(), False),\n",
    "    StructField(\"user_library\", StringType(), False)\n",
    "])\n",
    "\n",
    "users_libraries = spark.read.csv('citeulike/users_libraries.txt', sep = \";\", header = False, schema = usersLibrariesSchema)\n",
    "# key - user_hash, value - list of doc ids\n",
    "users_libraries = users_libraries.selectExpr('user_hash_id', 'split(user_library,\",\") AS user_library')\n",
    "\n",
    "execution_time_file.write('Loading of papers terms into DF: --- %s seconds --- \\n' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.6 Tasks on top of DataFrames\n",
    "Solve the tasks 6.3 and 6.4 again using DataFrames instead of RDDs. Record the execution times for each\n",
    "task and data model. Is there any noticeable performance difference between RDDs and DataFrames?\n",
    "Justify your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the following information:\n",
    "* number of (distinct) user, number of (distinct) items, and number of ratings\n",
    "* min number of ratings a user has given\n",
    "* max number of ratings a user has given\n",
    "* average number of ratings of users\n",
    "* standard deviation for ratings of users\n",
    "* min number of ratings an item has received\n",
    "* max number of ratings an item has received\n",
    "* average number of ratings of items\n",
    "* standard deviation for ratings of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28416\n",
      "172079\n",
      "+------------+\n",
      "|sum(ratings)|\n",
      "+------------+\n",
      "|      828481|\n",
      "+------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|           ratings|\n",
      "+-------+------------------+\n",
      "|  count|             28416|\n",
      "|   mean|29.155440596846848|\n",
      "| stddev| 81.17660451011594|\n",
      "|    min|                 1|\n",
      "|    max|              1922|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|            count|\n",
      "+-------+-----------------+\n",
      "|  count|           172079|\n",
      "|   mean| 4.81453867119172|\n",
      "| stddev|5.477818208917285|\n",
      "|    min|                3|\n",
      "|    max|              924|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.4 Basic Analysis\n",
    "start_time = time.time()\n",
    "# number of users\n",
    "print(users_libraries.count())\n",
    "\n",
    "# number of papers\n",
    "print(papers.count())\n",
    "\n",
    "# calculate #ratings per user - one column with a number of rated items per user\n",
    "ratings = users_libraries.select(F.size(users_libraries.user_library).alias(\"ratings\")) \n",
    "\n",
    "# number of total ratings\n",
    "ratings.groupBy().sum('ratings').show()\n",
    "\n",
    "# min, max, stddev, avg\n",
    "ratings.describe().show()\n",
    "\n",
    "# ratings.groupBy().max('ratings').show()\n",
    "# ratings.groupBy().min('ratings').show()\n",
    "# ratings.groupBy().avg('ratings').show()\n",
    "# ratings.groupBy().stddev('ratings').show()\n",
    "\n",
    "# calculate #ratings per item\n",
    "user_items = users_libraries.select(F.explode(users_libraries.user_library).alias(\"items\"))\n",
    "items_count = user_items.groupBy(\"items\").count()\n",
    "items_count.select(\"count\").describe().show()\n",
    "\n",
    "execution_time_file.write('Basic Analysis over DF: --- %s seconds --- \\n' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Joining\n",
    "start_time = time.time()\n",
    "\n",
    "papers = papers.filter(papers.abstract.isNotNull()).select('paper_id', 'abstract')\n",
    "users_papers = users_libraries.select('user_hash_id', F.explode(users_libraries.user_library).alias('paper_id'))\n",
    "user_abstracts = users_papers.join(papers, users_papers.paper_id == papers.paper_id).select('user_hash_id', 'abstract')\n",
    "\n",
    "# key - user_hash, value a word\n",
    "user_words = user_abstracts.selectExpr('user_hash_id', 'explode(split(abstract,\" \")) AS word')\n",
    "\n",
    "# remove stop words\n",
    "user_words = user_words.filter(user_words.word.isin(stopWords) == False)     \n",
    "\n",
    "# for each user and each word, count how many times it occurs\n",
    "user_words = user_words.groupBy('user_hash_id', 'word').agg(F.count(\"*\").alias('word_occurance'))\n",
    "user_words_window = Window.partitionBy(user_words.user_hash_id).orderBy(col('word_occurance').desc())\n",
    "\n",
    "# take first 10 words per user\n",
    "user_top_10_words = user_words.select(user_words.user_hash_id, user_words.word, rank().over(user_words_window).alias('rank')).filter(col('rank') <= 10).select(user_words.user_hash_id, user_words.word) \n",
    "\n",
    "# format into <user_hash> word_k word_k-1 ... word_1\n",
    "grouped_user_top_10_words = user_top_10_words.groupBy(user_words.user_hash_id).agg(F.collect_set('word').alias('words'))\n",
    "\n",
    "execution_time_file.write('Preparation and joining of DF: --- %s seconds --- \\n' % (time.time() - start_time)) \n",
    "\n",
    "\n",
    "# save into text file\n",
    "start_time = time.time()\n",
    "grouped_user_top_10_words.write.save(\"user_top_10_df\")\n",
    "execution_time_file.write('Saving the joined DFs: --- %s seconds --- \\n' % (time.time() - start_time)) \n",
    "\n",
    "execution_time_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another solution for removing the stop words is with join\n",
    "#user_words = user_words.join(stopWords, user_words.word == stopWords._1, \"left_outer\")\n",
    "#user_words = user_words.filter(stopWords._1.isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional calculations which are not part of the ex_sheet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load authors\n",
    "authors = spark.read.csv(\"citeulike/authors.csv\", header=True)\n",
    "\n",
    "# load keywords\n",
    "keywordsSchema = StructType([\n",
    "    #  name, dataType, nullable\n",
    "    StructField(\"citeulike_doc_id\", IntegerType(), False),\n",
    "    StructField(\"keyword\", StringType(), False)\n",
    "])\n",
    "\n",
    "keywords = spark.read.csv(\"citeulike/keywords.csv\", header=False, schema = keywordsSchema)\n",
    "\n",
    "# load papers vocabularies\n",
    "papers_vocab = sc.textFile(\"citeulike/papers_vocab.txt\")\n",
    "# remove the header\n",
    "header = papers_vocab.first()\n",
    "papers_vocab= papers_vocab.filter(lambda line: line != header)\n",
    "\n",
    "papers_vocab = papers_vocab.map(lambda k: k.split(\",\"))\n",
    "papers_vocab = papers_vocab.map(lambda x: (x[0], parse_papers_count(x[1])))\n",
    "\n",
    "papersVocSchema = StructType([\n",
    "    #  name, dataType, nullable\n",
    "    StructField(\"citeulike_doc_id\", IntegerType(), False),\n",
    "    StructField(\"vocabularies\", ArrayType(StringType(), False), True),\n",
    "])\n",
    "papers_vocab = papers_vocab.toDF(papersVocSchema)\n",
    "\n",
    "# load vocabularies\n",
    "vocabs = sc.textFile('citeulike/vocab.txt')\n",
    "vocabs = vocabs.map(lambda k: k.split(\"\\t\")).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nullable features calculation\n",
    "print(papers.filter(papers.type.isNull()).count())\n",
    "print(papers.filter(papers.journal.isNull()).count())\n",
    "print(papers.filter(papers.book_title.isNull()).count())\n",
    "print(papers.filter(papers.series.isNull()).count())\n",
    "print(papers.filter(papers.publisher.isNull()).count())\n",
    "print(papers.filter(papers.pages.isNull()).count())\n",
    "print(papers.filter(papers.volume.isNull()).count())\n",
    "print(papers.filter(papers.number.isNull()).count())\n",
    "print(papers.filter(papers.year.isNull()).count())\n",
    "print(papers.filter(papers.month.isNull()).count())\n",
    "print(papers.filter(papers.postedat.isNull()).count())\n",
    "print(papers.filter(papers.address.isNull()).count())\n",
    "print(papers.filter(papers.title.isNull()).count())\n",
    "print(papers.filter(papers.abstract.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
